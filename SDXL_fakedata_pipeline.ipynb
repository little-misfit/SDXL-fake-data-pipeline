{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lora Trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-processing\n",
    "In order to use diffusers for lora training, a specific dataset format should be built.More details: https://huggingface.co/docs/diffusers/training/create_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from lavis.models import load_model_and_preprocess\n",
    "\n",
    "#blip2 environment: https://github.com/salesforce/LAVIS/tree/main/projects/blip2\n",
    "\n",
    "\n",
    "# setup device to use\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#image caption by blip2\n",
    "model, vis_processors, _ = load_model_and_preprocess(\n",
    "    name=\"blip2_opt\", model_type=\"pretrain_opt6.7b\", is_eval=True, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caption function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def img2txt(image):\n",
    "    image = vis_processors[\"eval\"](image).unsqueeze(0).to(device)\n",
    "    txt = model.generate({\"image\":image,\"prompt\":\"a clothing photo of\"})\n",
    "    # txt = model.generate({\"image\": image, \"prompt\": \"Question: Please describe the following aspects of the clothing in the image, \"+ \n",
    "    #             \"1.Style and Design: Detail the overall style and any distinctive design features. \" + \n",
    "    #             \"2.Color and Pattern: Describe the primary color(s) and any patterns or prints present. \" +\n",
    "    #             \"3.Fabric and Material: Identify the type of fabric and material quality. \" +\n",
    "    #             \"4.Size and Fit: Comment on the size, fit, and cut of the clothing. \" +\n",
    "    #             \"5.Details and Embellishments: Note any specific details or decorative elements. \" +\n",
    "    #             \"Appropriate Occasions: Suggest occasions or settings where this clothing would be suitable to wear. Answer:\"})\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caption!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import json\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "img_path = '/home/sd/Harddisk/ZXP/DressCode/images'\n",
    "out_path = '/home/sd/Harddisk/ZXP/SDXL_LOra/Data_set/DressCode'\n",
    "\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "    \n",
    "imgs = os.listdir(img_path)\n",
    "print(len(imgs)/2)\n",
    "time.sleep(3)\n",
    "captions = []\n",
    "for img_name in tqdm.tqdm(imgs):\n",
    "    if img_name[-5] != '1':\n",
    "        continue\n",
    "    img = Image.open(os.path.join(img_path,img_name)).convert('RGB')\n",
    "    txt = img2txt(img)\n",
    "    caption = {'file_name':img_name,'text':txt[0]}\n",
    "    captions.append(caption)\n",
    "\n",
    "with open(os.path.join(out_path,'metadata.jsonl'),'w') as file:\n",
    "    for caption in captions:\n",
    "        json_line = json.dumps(caption)\n",
    "        file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lora trainning\n",
    "Use the training script provided by diffusers for training. For specific tutorials, refer to: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md\n",
    "Attention: the train_text_to_image_lora_sdxl.py used in this notebook has be editted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "!export VAE_NAME=\"madebyollin/sdxl-vae-fp16-fix\"\n",
    "!export DATASET_NAME=\"/home/sd/Harddisk/ZXP/SDXL_LOra/Data_set/test/cloth\"\n",
    "\n",
    "!accelerate launch diffusers/examples/text_to_image/train_text_to_image_lora_sdxl.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --pretrained_vae_model_name_or_path=$VAE_NAME \\\n",
    "  --caption_column=\"text\" \\\n",
    "  --resolution=1024 --random_flip \\\n",
    "  --train_data_dir /home/sd/Harddisk/ZXP/SDXL_LOra/Data_set \\\n",
    "  --train_batch_size=2 \\\n",
    "  --num_train_epochs=4 --checkpointing_steps=5000 \\\n",
    "  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --seed=42 \\\n",
    "  --output_dir=\"sd-finetune-model-lora-sdxl\" \\\n",
    "  --validation_prompt=\"a photo of blue dress\" --report_to=\"wandb\" \\\n",
    "  --push_to_hub \\\n",
    "  --validation_steps 2500 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lora inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_path = \"/home/sd/Harddisk/ZXP/SDXL_LOra/sd-finetune-model-lora-sdxl/checkpoint-50000\"\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16)\n",
    "pipe.to(\"cuda\")\n",
    "pipe.load_lora_weights(model_path)\n",
    "\n",
    "prompt = \"a photo of blue jeans skirt, White background\"\n",
    "image = pipe(prompt, num_inference_steps=50, \n",
    "             guidance_scale=2.5                   #text prompt guidance scale\n",
    "             ).images[0]\n",
    "image.save(\"Lora_output/test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Lora inference (Not compatible with other adapters, for now.) More details: https://huggingface.co/docs/diffusers/main/en/tutorials/using_peft_for_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "pipe_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "pipe = DiffusionPipeline.from_pretrained(pipe_id, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "#lora No.1\n",
    "pipe.load_lora_weights(\"CiroN2022/toy-face\", weight_name=\"toy_face_sdxl.safetensors\", adapter_name=\"toy\")\n",
    "#lora No.2\n",
    "pipe.load_lora_weights(\"nerijs/pixel-art-xl\", weight_name=\"pixel-art-xl.safetensors\", adapter_name=\"pixel\")\n",
    "#lora fuse\n",
    "pipe.set_adapters([\"pixel\", \"toy\"], adapter_weights=[0.5, 1.0])\n",
    "\n",
    "#inference\n",
    "prompt = \"toy_face of a hacker with a hoodie\"\n",
    "lora_scale= 0.9\n",
    "image = pipe(\n",
    "    prompt, num_inference_steps=30, \n",
    "    cross_attention_kwargs={\"scale\": lora_scale},    #lora scale \n",
    "    generator=torch.manual_seed(0)\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2I Adapter(ControlNet) Trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainning will upload soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference(Lora + T2I-Adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, DiffusionPipeline\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "import peft\n",
    "\n",
    "base_model_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "adapter_path = \"/home/sd/Harddisk/ZXP/SDXL_LOra/T2IAdapter/openpose\"\n",
    "model_path = \"/home/sd/Harddisk/ZXP/SDXL_LOra/sd-pokemon-model-lora-sdxl/checkpoint-50000\"\n",
    "\n",
    "adapter = T2IAdapter.from_pretrained(adapter_path, torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n",
    "    base_model_path, adapter=adapter, torch_dtype=torch.float16\n",
    ")\n",
    "pipe.load_lora_weights(model_path,adapter_name=\"black_ground_clothing\")\n",
    "# pipe.set_adapters(\"black_ground_clothing\")\n",
    "# speed up diffusion process with faster scheduler and memory optimization\n",
    "# pipe.scheduler = EulerAncestralDiscreteSchedulerTest.from_config(pipe.scheduler.config)\n",
    "# remove following line if xformers is not installed or when using Torch 2.0.\n",
    "# pipe.enable_xformers_memory_efficient_attention()\n",
    "# memory optimization.\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "control_image = load_image(\"/home/sd/Harddisk/ZXP/DressCode/skeletons/000003_5.jpg\")\n",
    "prompt = \"a photo of a blue off the shoulder top, White background\"\n",
    "\n",
    "# generate image\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt, num_inference_steps=50, \n",
    "    cross_attention_kwargs={\"scale\": 1},               #lora_scale \n",
    "    generator=generator, image=control_image, \n",
    "    guidance_scale=2.5,                                #prompt_scale\n",
    "    adapter_conditioning_scale=0.8                     #T2I-adapter_scale\n",
    ").images[0] \n",
    "image"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
